{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelaestefan/concentracion/blob/master/ejemplos_tema3_svm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b445ca5",
      "metadata": {
        "id": "2b445ca5"
      },
      "source": [
        "Esta libreta es una traducción de:\n",
        "https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.07-Support-Vector-Machines.ipynb#scrollTo=Mu3PR3aMH64U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4350911c",
      "metadata": {
        "id": "4350911c"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f72455",
      "metadata": {
        "id": "78f72455"
      },
      "source": [
        "## Máquinas de vectores de soporte - Motivación\n",
        "Para comenzar, suponga que tenemos una tarea de clasificación en el que los dos tipos de clases están bien separados.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dca35ad9",
      "metadata": {
        "id": "dca35ad9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "X, y = make_blobs(n_samples=50, centers=2,\n",
        "                  random_state=0, cluster_std=0.60)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac402c4e",
      "metadata": {
        "id": "ac402c4e"
      },
      "source": [
        "Un clasificador discriminativo lineal intentaría dibujar una línea recta que separara los dos conjuntos de datos y, de este modo, crear un modelo de clasificación. Para datos bidimensionales como los que se muestran aquí, esta tarea podría realizarse manualmente. Pero inmediatamente surge un problema: ¡existe más de una línea divisoria posible que puede discriminar perfectamente entre las dos clases!\n",
        "\n",
        "Podemos dibujar algunas de ellas de la siguiente manera; la siguiente figura muestra el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d4d9aa3",
      "metadata": {
        "id": "4d4d9aa3"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plt.plot([0.6], [2.1], 'x', color='red', markeredgewidth=2, markersize=10)\n",
        "\n",
        "for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:\n",
        "    plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "939b7f0a",
      "metadata": {
        "id": "939b7f0a"
      },
      "source": [
        "Se trata de tres separadores *muy* diferentes que, sin embargo, discriminan perfectamente entre estas muestras.\n",
        "Dependiendo de cuál elija, a un nuevo punto de datos (por ejemplo, el marcado con la \"X\" en este gráfico) se le asignará una etiqueta diferente.\n",
        "Evidentemente, nuestra simple intuición de \"trazar una línea entre clases\" no es suficiente, y necesitamos reflexionar con más profundidad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc1c5a92",
      "metadata": {
        "id": "dc1c5a92"
      },
      "source": [
        "# Máquinas de soporte de vectores: maximización del margen\n",
        "\n",
        "Las máquinas de vectores de soporte ofrecen una manera de mejorar esto. La intuición es la siguiente: en lugar de simplemente trazar una línea de ancho cero entre las clases, podemos trazar alrededor de cada línea un margen de cierto ancho, hasta el punto más cercano. Aquí tienes un ejemplo de cómo podría verse (ver la siguiente figura):”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63e5e5f",
      "metadata": {
        "id": "e63e5e5f"
      },
      "outputs": [],
      "source": [
        "xfit = np.linspace(-1, 3.5)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
        "    yfit = m * xfit + b\n",
        "    plt.plot(xfit, yfit, '-k')\n",
        "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
        "                     color='lightgray', alpha=0.5)\n",
        "\n",
        "plt.xlim(-1, 3.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79d58779",
      "metadata": {
        "id": "79d58779"
      },
      "source": [
        "La línea que maximice este margen es la que elegiremos como modelo óptimo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2a7c93",
      "metadata": {
        "id": "5f2a7c93"
      },
      "source": [
        "# Ajustando una máquina de soporte vectorial\n",
        "\n",
        "Veamos el resultado de un ajuste real a estos datos: utilizaremos el clasificador de vectores de soporte (SVC) de Scikit-Learn para entrenar un modelo SVM con estos datos. Por ahora, utilizaremos un kernel lineal y estableceremos el parámetro C en un valor muy alto (profundizaremos en su significado más adelante)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3331b58e",
      "metadata": {
        "id": "3331b58e"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier\"\n",
        "model = SVC(kernel='linear', C=1E10)\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88618a66",
      "metadata": {
        "id": "88618a66"
      },
      "source": [
        "\n",
        "Para visualizar mejor lo que sucede aquí, creemos una función de conveniencia rápida que trazará los límites de decisión de SVM para nosotros (vea la siguiente figura):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18bc8d9d",
      "metadata": {
        "id": "18bc8d9d"
      },
      "outputs": [],
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)\n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "\n",
        "    # plot decision boundary and margins\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "\n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, edgecolors='black',\n",
        "                   facecolors='none');\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "698a7700",
      "metadata": {
        "id": "698a7700"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97cb0dbc",
      "metadata": {
        "id": "97cb0dbc"
      },
      "source": [
        "Esta es la línea divisoria que maximiza el margen entre los dos conjuntos de puntos.\n",
        "Observe que algunos puntos de entrenamiento apenas tocan el margen: están rodeados con un círculo en la siguiente figura.\n",
        "Estos puntos son los elementos clave de este ajuste; se conocen como *vectores de soporte* y dan nombre al algoritmo.\n",
        "En Scikit-Learn, las identidades de estos puntos se almacenan en el atributo `support_vectors_` del clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7ece5f",
      "metadata": {
        "id": "1e7ece5f"
      },
      "outputs": [],
      "source": [
        "model.support_vectors_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d5fc8b",
      "metadata": {
        "id": "12d5fc8b"
      },
      "source": [
        "Una clave del éxito de este clasificador es que, para el ajuste, solo importan las posiciones de los vectores de soporte; los puntos más alejados del margen que estén en el lado correcto no modifican el ajuste. Técnicamente, esto se debe a que estos puntos no contribuyen a la función de pérdida utilizada para ajustar el modelo, por lo que su posición y número no importan siempre que no crucen el margen.\n",
        "\n",
        "Podemos comprobarlo, por ejemplo, si graficamos el modelo aprendido a partir de los primeros 60 y 120 puntos de este conjunto de datos (véase la siguiente figura):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1175159",
      "metadata": {
        "id": "c1175159"
      },
      "outputs": [],
      "source": [
        "def plot_svm(N=10, ax=None):\n",
        "    X, y = make_blobs(n_samples=200, centers=2,\n",
        "                      random_state=0, cluster_std=0.60)\n",
        "    X = X[:N]\n",
        "    y = y[:N]\n",
        "    model = SVC(kernel='linear', C=1E10)\n",
        "    model.fit(X, y)\n",
        "\n",
        "    ax = ax or plt.gca()\n",
        "    ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    ax.set_xlim(-1, 4)\n",
        "    ax.set_ylim(-1, 6)\n",
        "    plot_svc_decision_function(model, ax)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "for axi, N in zip(ax, [60, 120]):\n",
        "    plot_svm(N, axi)\n",
        "    axi.set_title('N = {0}'.format(N))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3680a248",
      "metadata": {
        "id": "3680a248"
      },
      "source": [
        "En el panel izquierdo, vemos el modelo y los vectores de soporte para 60 puntos de entrenamiento.\n",
        "En el panel derecho, hemos duplicado el número de puntos de entrenamiento, pero el modelo no ha cambiado: los tres vectores de soporte del panel izquierdo son los mismos que los del panel derecho.\n",
        "Esta insensibilidad al comportamiento exacto de los puntos distantes es una de las fortalezas del modelo SVM.\n",
        "\n",
        "Si está ejecutando este cuaderno en vivo, puede usar los widgets interactivos de IPython para ver esta característica del modelo SVM de forma interactiva:\n",
        "\n",
        "(Descomentar y correr las siguientes líneas, pero cuidado puede tardar mucho tiempo.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e217153a",
      "metadata": {
        "id": "e217153a"
      },
      "outputs": [],
      "source": [
        "# from ipywidgets import interact, fixed\n",
        "# interact(plot_svm, N=(10, 200), ax=fixed(None));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11e1c752",
      "metadata": {
        "id": "11e1c752"
      },
      "source": [
        "### Ajuste de la máquina de vectores de soporte: Margen blando\n",
        "\n",
        "Hasta ahora, nuestro análisis se ha centrado en conjuntos de datos muy limpios, en los que existe un límite de decisión perfecto.\n",
        "Pero ¿qué ocurre si sus datos presentan cierta superposición?\n",
        "Por ejemplo, podría tener datos como estos (véase la siguiente figura):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44700ac7",
      "metadata": {
        "id": "44700ac7"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=1.2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c0ea5c8",
      "metadata": {
        "id": "8c0ea5c8"
      },
      "source": [
        "Para abordar este caso, la implementación de SVM cuenta con un factor de ajuste que \"suaviza\" el margen: es decir, permite que algunos puntos se deslicen hacia el margen si esto facilita un mejor ajuste.\n",
        "La dureza del margen se controla mediante un parámetro de ajuste, comúnmente conocido como `C`.\n",
        "Para una `C` muy grande, el margen es duro y los puntos no pueden estar en él.\n",
        "Para una `C` más pequeña, el margen es más suave y puede crecer hasta abarcar algunos puntos.\n",
        "\n",
        "La gráfica que se muestra en la siguiente figura muestra cómo un cambio en la `C` afecta el ajuste final mediante la suavización del margen:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7d6e235",
      "metadata": {
        "id": "b7d6e235"
      },
      "outputs": [],
      "source": [
        "X, y = make_blobs(n_samples=100, centers=2,\n",
        "                  random_state=0, cluster_std=0.8)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
        "\n",
        "for axi, C in zip(ax, [10.0, 0.1]):\n",
        "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
        "    axi.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "    plot_svc_decision_function(model, axi)\n",
        "    axi.scatter(model.support_vectors_[:, 0],\n",
        "                model.support_vectors_[:, 1],\n",
        "                s=300, lw=1, facecolors='none');\n",
        "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "175efa3c",
      "metadata": {
        "id": "175efa3c"
      },
      "source": [
        "\n",
        "El valor óptimo de \"C\" dependerá de su conjunto de datos, y debe ajustar este parámetro mediante validación cruzada o un procedimiento similar (consulte [Hiperparámetros y validación del modelo](05.03-Hyperparameters-and-Model-Validation.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e73cd69c",
      "metadata": {
        "id": "e73cd69c"
      },
      "source": [
        "### Más allá de los límites lineales: SVM de kernel\n",
        "\n",
        "El SVM puede ser muy eficaz al combinarse con *kernels*. La idea es proyectar los datos a un espacio de mayor dimensionalidad donde los datos puedan ser linealmente serapables. Para justificar la necesidad de kernels, veamos algunos datos que no son linealmente separables (véase la siguiente figura):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9096c6",
      "metadata": {
        "id": "fc9096c6"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(100, factor=.1, noise=.1)\n",
        "\n",
        "clf = SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf, plot_support=False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4665521",
      "metadata": {
        "id": "d4665521"
      },
      "source": [
        "Es evidente que ninguna discriminación lineal podrá separar estos datos.\n",
        "Pero podemos pensar en cómo podríamos proyectar los datos a una dimensión superior de modo que un separador lineal sea suficiente.\n",
        "Por ejemplo, una proyección sencilla que podríamos usar sería calcular una *función base radial* (RBF) centrada en el grupo central:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b820a4",
      "metadata": {
        "id": "09b820a4"
      },
      "outputs": [],
      "source": [
        "r = np.exp(-(X ** 2).sum(1))\n",
        "print(\"Shape X: \", X.shape)\n",
        "print(\"Shape r: \", r.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bb6e90",
      "metadata": {
        "id": "e1bb6e90"
      },
      "source": [
        "Podemos visualizar esta dimensión de datos adicionales utilizando un gráfico tridimensional, como se ve en la siguiente figura:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b54d4f5",
      "metadata": {
        "id": "5b54d4f5"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "ax = plt.subplot(projection='3d')\n",
        "ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
        "ax.view_init(elev=20, azim=30)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('r');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f0a3c7d",
      "metadata": {
        "id": "1f0a3c7d"
      },
      "source": [
        "Podemos observar que, con esta dimensión adicional, los datos se vuelven fácilmente separables linealmente, dibujando un plano de separación en, por ejemplo, *r*=0,7.\n",
        "\n",
        "En este caso, tuvimos que elegir y ajustar cuidadosamente nuestra proyección: si no hubiéramos centrado nuestra función de base radial en la ubicación correcta, no habríamos obtenido resultados tan limpios y linealmente separables.\n",
        "En general, la necesidad de hacer tal elección es un problema: nos gustaría encontrar de alguna manera automáticamente las mejores funciones de base para usar.\n",
        "\n",
        "Una estrategia para este fin es calcular una función de base centrada en *cada* punto del conjunto de datos y dejar que el algoritmo SVM filtre los resultados.\n",
        "Este tipo de transformación de la función de base se conoce como *transformación kernel*, ya que se basa en una relación de similitud (o kernel) entre cada par de puntos.\n",
        "\n",
        "Un problema potencial con esta estrategia (proyectar $N$ puntos en $N$ dimensiones) es que podría volverse muy intensiva computacionalmente a medida que $N$ aumenta. Sin embargo, gracias a un sencillo procedimiento conocido como el [*truco del kernel*](https://en.wikipedia.org/wiki/Kernel_trick), se puede realizar un ajuste en datos transformados por kernel de forma implícita, es decir, sin construir la representación completa de $N$ dimensiones de la proyección del kernel.\n",
        "Este truco del kernel está integrado en la SVM y es una de las razones por las que el método es tan eficaz.\n",
        "\n",
        "En Scikit-Learn, podemos aplicar la SVM kernelizada simplemente cambiando nuestro kernel lineal a un kernel RBF, utilizando el hiperparámetro del modelo `kernel`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "527f540d",
      "metadata": {
        "id": "527f540d"
      },
      "outputs": [],
      "source": [
        "clf = SVC(kernel='rbf', C=1E6)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfbad582",
      "metadata": {
        "id": "bfbad582"
      },
      "source": [
        "Let's use our previously defined function to visualize the fit and identify the support vectors (see the following figure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2da0b6f",
      "metadata": {
        "id": "d2da0b6f"
      },
      "outputs": [],
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(clf)\n",
        "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
        "            s=300, lw=1, facecolors='none');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "071615c7",
      "metadata": {
        "id": "071615c7"
      },
      "source": [
        "Utilizando esta máquina de vectores de soporte kernelizada, aprendemos un límite de decisión no lineal adecuado.\n",
        "Esta estrategia de transformación kernel se utiliza a menudo en el aprendizaje automático para convertir métodos lineales rápidos en métodos no lineales rápidos, especialmente para modelos en los que se puede utilizar el truco kernel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c85fb66b",
      "metadata": {
        "id": "c85fb66b"
      },
      "source": [
        "### Ejemplo: Reconocimiento facial\n",
        "\n",
        "Como ejemplo de máquinas de vectores de soporte en acción, analicemos el problema del reconocimiento facial.\n",
        "Utilizaremos el conjunto de datos \"Rostros etiquetados en la naturaleza\", que consta de miles de fotos recopiladas de diversas figuras públicas.\n",
        "Scikit-Learn integra un buscador para el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa3b56a",
      "metadata": {
        "id": "5aa3b56a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_lfw_people\n",
        "faces = fetch_lfw_people(min_faces_per_person=60)\n",
        "print(faces.target_names)\n",
        "print(faces.images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7942768",
      "metadata": {
        "id": "b7942768"
      },
      "source": [
        "Vamos a graficar algunas muestras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8973c0da",
      "metadata": {
        "id": "8973c0da"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, ax = plt.subplots(3, 5, figsize=(8, 6))\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(faces.images[i], cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[],\n",
        "            xlabel=faces.target_names[faces.target[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7068ada",
      "metadata": {
        "id": "e7068ada"
      },
      "source": [
        "Cada imagen contiene 62 × 47, o aproximadamente 3000 píxeles. Podríamos proceder simplemente usando cada valor de píxel como una característica, pero a menudo resulta más efectivo usar algún tipo de preprocesador para extraer características más significativas. En este caso, utilizaremos el análisis de componentes principales (véase \"A fondo: Análisis de componentes principales\") para extraer 150 componentes fundamentales que alimentaremos a nuestro clasificador de máquinas de vectores de soporte. Podemos hacerlo de forma más sencilla empaquetando el preprocesador y el clasificador en una sola canalización:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ef50bb",
      "metadata": {
        "id": "34ef50bb"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "pca = PCA(n_components=150, whiten=True,\n",
        "          svd_solver='randomized', random_state=42)\n",
        "svc = SVC(kernel='rbf', class_weight='balanced')\n",
        "model = make_pipeline(pca, svc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c81f021",
      "metadata": {
        "id": "4c81f021"
      },
      "source": [
        "Para probar la salida de nuestro clasificador, dividiremos los datos en un conjunto de entrenamiento y un conjunto de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "917032f2",
      "metadata": {
        "id": "917032f2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data, faces.target,\n",
        "                                                random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300a95f9",
      "metadata": {
        "id": "300a95f9"
      },
      "source": [
        "Finalmente, podemos usar la validación cruzada de búsqueda en cuadrícula para explorar combinaciones de parámetros.\n",
        "Aquí ajustaremos ``C`` (que controla la dureza del margen) y ``gamma`` (que controla el tamaño del kernel de la función de base radial) y determinaremos el mejor modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "692d610d",
      "metadata": {
        "id": "692d610d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'svc__C': [1, 5, 10, 50],\n",
        "              'svc__gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
        "grid = GridSearchCV(model, param_grid)\n",
        "\n",
        "%time grid.fit(Xtrain, ytrain)\n",
        "print(grid.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee25a8c",
      "metadata": {
        "id": "1ee25a8c"
      },
      "source": [
        "Los valores óptimos se encuentran hacia el centro de nuestra cuadrícula; si se encuentran en los extremos, sería conveniente expandir la cuadrícula para asegurarnos de haber encontrado el valor óptimo real.\n",
        "\n",
        "Ahora, con este modelo de validación cruzada, podemos predecir las etiquetas de los datos de prueba, que el modelo aún no ha detectado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d15d33",
      "metadata": {
        "id": "c8d15d33"
      },
      "outputs": [],
      "source": [
        "model = grid.best_estimator_\n",
        "yfit = model.predict(Xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a37f9ee0",
      "metadata": {
        "id": "a37f9ee0"
      },
      "source": [
        "Echemos un vistazo a algunas de las imágenes de prueba junto con sus valores previstos (consulte la siguiente figura):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8cef94f",
      "metadata": {
        "id": "d8cef94f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(4, 6)\n",
        "for i, axi in enumerate(ax.flat):\n",
        "    axi.imshow(Xtest[i].reshape(62, 47), cmap='bone')\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],\n",
        "                   color='black' if yfit[i] == ytest[i] else 'red')\n",
        "fig.suptitle('Predicted Names; Incorrect Labels in Red', size=14);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd30ed30",
      "metadata": {
        "id": "bd30ed30"
      },
      "source": [
        "Podemos comprender mejor el rendimiento de nuestro estimador utilizando el informe de clasificación, que muestra las estadísticas de recuperación etiqueta por etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0669f459",
      "metadata": {
        "id": "0669f459"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(ytest, yfit,\n",
        "                            target_names=faces.target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44f7516",
      "metadata": {
        "id": "c44f7516"
      },
      "source": [
        "También podemos mostrar la matriz de confusión entre estas clases (ver la siguiente figura):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a979b27d",
      "metadata": {
        "id": "a979b27d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "mat = confusion_matrix(ytest, yfit)\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d',\n",
        "            cbar=False, cmap='Blues',\n",
        "            xticklabels=faces.target_names,\n",
        "            yticklabels=faces.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f987161",
      "metadata": {
        "id": "3f987161"
      },
      "source": [
        "Esto nos ayuda a comprender qué etiquetas podrían ser confundidas por el estimador.\n",
        "\n",
        "Para una tarea de reconocimiento facial real, donde las fotos no vienen precortadas en cuadrículas definidas, la única diferencia en el esquema de clasificación facial es la selección de características: se necesitaría un algoritmo más sofisticado para encontrar los rostros y extraer características independientes de la pixelación. Para este tipo de aplicación, una buena opción es usar OpenCV, que, entre otras cosas, incluye implementaciones preentrenadas de herramientas de extracción de características de vanguardia para imágenes en general y rostros en particular."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7662f46",
      "metadata": {
        "id": "b7662f46"
      },
      "source": [
        "## Resumen\n",
        "\n",
        "Esta ha sido una breve introducción intuitiva a los principios de las máquinas de vectores de soporte.\n",
        "Estos modelos constituyen un potente método de clasificación por varias razones:\n",
        "\n",
        "- Su dependencia de relativamente pocos vectores de soporte significa que son compactos y ocupan muy poca memoria.\n",
        "- Una vez entrenado el modelo, la fase de predicción es muy rápida.\n",
        "- Dado que solo se ven afectados por los puntos cercanos al margen, funcionan bien con datos de alta dimensión, incluso con datos con más dimensiones que las muestras, lo cual supone un reto para otros algoritmos.\n",
        "- Su integración con los métodos kernel los hace muy versátiles, capaces de adaptarse a diversos tipos de datos.\n",
        "\n",
        "Sin embargo, las SVM también presentan varias desventajas:\n",
        "\n",
        "- El escalado con el número de muestras $N$ es $\\mathcal{O}[N^3]$ en el peor de los casos, o $\\mathcal{O}[N^2]$ para implementaciones eficientes. Para un gran número de muestras de entrenamiento, este coste computacional puede ser prohibitivo.\n",
        "\n",
        "Los resultados dependen en gran medida de la elección adecuada del parámetro de suavizado «C». Este debe seleccionarse cuidadosamente mediante validación cruzada, lo cual puede resultar costoso a medida que aumentan el tamaño de los conjuntos de datos.\n",
        "Los resultados no tienen una interpretación probabilística directa. Esto puede estimarse mediante una validación cruzada interna (véase el parámetro «probabilidad» de «SVC»), pero esta estimación adicional es costosa.\n",
        "\n",
        "Teniendo en cuenta estas características, generalmente solo recurro a las SVM cuando otros métodos más sencillos, rápidos y que requieren menos ajustes han demostrado ser insuficientes para mis necesidades.\n",
        "Sin embargo, si dispone de los ciclos de CPU necesarios para entrenar y validar una SVM con sus datos, el método puede ofrecer excelentes resultados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}