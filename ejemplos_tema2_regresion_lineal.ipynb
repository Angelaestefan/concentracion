{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angelaestefan/concentracion/blob/master/ejemplos_tema2_regresion_lineal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "310ffaaf",
      "metadata": {
        "id": "310ffaaf"
      },
      "source": [
        "### Regresión lineal simple con solución cerrada (Ecuaciones normales)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9b56846",
      "metadata": {
        "id": "b9b56846"
      },
      "source": [
        "$$ w = \\frac{\\sum_{i=1}^n x_i y_i - \\frac{1}{n}\\left(\\sum_{i=1}^n x_i\\right)\\left(\\sum_{i=1}^n y_i\\right)}{\\sum_{i=1}^n x_i^2 - \\frac{1}{n}\\left(\\sum_{i=1}^n x_i\\right)^2} $$\n",
        "\n",
        "$$ b = \\frac{1}{n}\\sum_{i=1}^n y_i - \\frac{w}{n}\\sum_{i=1}^n x_i $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36cb3a78",
      "metadata": {
        "id": "36cb3a78"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generar datos\n",
        "x = np.random.uniform(0, 10, 100)\n",
        "y = 5 + 2*x + np.random.normal(0, 1, 100)\n",
        "\n",
        "# Calcular coeficientes del modelo (closed-form)\n",
        "n = len(x)\n",
        "w = ((x*y).sum() - (1./n)*x.sum()*y.sum()) / ((x*x).sum() - (1./n)*(x.sum()**2))\n",
        "b = (1./n)*y.sum() - (w/n)*x.sum()\n",
        "print(\"Modelo: y =\", b, \"+\", w, \"* x\")\n",
        "\n",
        "# Predicciones y residuos\n",
        "y_pred = w*x + b\n",
        "r = y - y_pred\n",
        "\n",
        "# ---- Tres subplots ----\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Datos originales\n",
        "axes[0].scatter(x, y)\n",
        "axes[0].set_xlabel('x')\n",
        "axes[0].set_ylabel('y')\n",
        "axes[0].set_title('Datos')\n",
        "\n",
        "# línea de regression\n",
        "idx = np.argsort(x)\n",
        "axes[1].scatter(x, y)\n",
        "axes[1].plot(x[idx], y_pred[idx], color='red', linewidth=2)\n",
        "axes[1].set_xlabel('x')\n",
        "axes[1].set_ylabel('y')\n",
        "axes[1].set_title('Modelo')\n",
        "\n",
        "# Residuos\n",
        "axes[2].scatter(y, r)\n",
        "axes[2].axhline(0, color='red', linestyle='--', linewidth=2)\n",
        "axes[2].set_xlabel('y')\n",
        "axes[2].set_ylabel('Error')\n",
        "axes[2].set_title('Residuos')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2a751a6",
      "metadata": {
        "id": "d2a751a6"
      },
      "source": [
        "# Dataset diabetes\n",
        "\n",
        "Diabetes dataset\n",
        "442 muestras (filas), 10 caracteristicas (columnas)\n",
        "\n",
        "Características:\n",
        "- age – Edad del paciente.\n",
        "- sex – Sexo del paciente.\n",
        "- bmi – Índice de masa corporal (body mass index).\n",
        "- bp – Presión arterial promedio.\n",
        "- s1 – Medida de colesterol sérico.\n",
        "- s2 – LDL (lipoproteínas de baja densidad).\n",
        "- s3 – HDL (lipoproteínas de alta densidad).\n",
        "- s4 – Relación de colesterol total con HDL.\n",
        "- s5 – Nivel de triglicéridos en sangre.\n",
        "- s6 – Nivel de glucosa en sangre.\n",
        "\n",
        "Variable objetivo:\n",
        "\n",
        "Una medida cuantitativa de la progresión de la diabetes un año después de la primera observación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d91578",
      "metadata": {
        "id": "c4d91578"
      },
      "source": [
        "### Caso 1. Entrenamiento y validación con el mismo conjunto de datos (No recomendado, sobreestima el desempeño)\n",
        "\n",
        "La siguiente celda entrena y valida un modelo de regresion lineal utilizando el mismo conjunto de datos. Esto no es recomendado, ya que la estimación del desempeño es optimista."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c29b91a",
      "metadata": {
        "id": "0c29b91a"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "print(f\"# features: {n_features}\")\n",
        "\n",
        "regr = LinearRegression()\n",
        "regr.fit(x, y)\n",
        "\n",
        "print(\"Coeficientes del modelo: \\n\", regr.coef_)\n",
        "\n",
        "y_pred = regr.predict(x)\n",
        "print('MSE: \\n', mean_squared_error(y, y_pred))\n",
        "print('MAE: \\n', mean_absolute_error(y, y_pred))\n",
        "print(\"R^2: \\n\", r2_score(y, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8163ac83",
      "metadata": {
        "id": "8163ac83"
      },
      "source": [
        "### Caso 2. Entrena y evalúa en conjuntos separados (train, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0196c8e",
      "metadata": {
        "id": "a0196c8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "# 1. Load dataset\n",
        "diabetes = load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "print(f\"# features: {n_features}\")\n",
        "\n",
        "# 2. Dividir en train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    x, y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# 4. Entrenar el modelo\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predicciones y evaluación\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Intercepto:\", model.intercept_)\n",
        "for name, coef in zip(features, model.coef_):\n",
        "    print(f\"{name}: {coef:.4f}\")\n",
        "print('-'*10)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
        "print(\"R²:\", r2_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84503b66",
      "metadata": {
        "id": "84503b66"
      },
      "source": [
        "### Caso 3. Utiliza K-Fold validación cruzada en un bucle manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a917b7d",
      "metadata": {
        "id": "9a917b7d"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "# 1. Load dataset\n",
        "diabetes = load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "print(f\"# features: {n_features}\")\n",
        "\n",
        "n_folds = 5\n",
        "kf = KFold(n_splits=n_folds, shuffle = True, random_state=0)\n",
        "\n",
        "mse = 0\n",
        "mae = 0\n",
        "r2 = 0\n",
        "for k, (train_index, test_index) in enumerate(kf.split(x)):\n",
        "    print(f'Iteración de k-fold: {k+1}')\n",
        "    # Training phase\n",
        "    x_train = x[train_index, :]\n",
        "    y_train = y[train_index]\n",
        "\n",
        "    regr_cv = LinearRegression()\n",
        "    regr_cv.fit(x_train, y_train)\n",
        "\n",
        "    # Test phase\n",
        "    x_test = x[test_index, :]\n",
        "    y_test = y[test_index]\n",
        "\n",
        "    y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "    # Calculate MSE and R^2\n",
        "    mse_i = mean_squared_error(y_test, y_pred)\n",
        "    print('\\t mse = ', mse_i)\n",
        "\n",
        "    mae_i = mean_absolute_error(y_test, y_pred)\n",
        "    print('\\t mae = ', mae_i)\n",
        "\n",
        "    r2_i = r2_score(y_test, y_pred)\n",
        "    print('\\t r^2= ', r2_i)\n",
        "\n",
        "    mse += mse_i\n",
        "    mae += mae_i\n",
        "    r2 += r2_i\n",
        "\n",
        "\n",
        "print(\"\\nMetricas promedio: \\n\")\n",
        "\n",
        "mse = mse/n_folds\n",
        "print('MSE = ', mse)\n",
        "\n",
        "mae = mae/n_folds\n",
        "print('MAE = ', mae)\n",
        "\n",
        "r2 = r2/n_folds\n",
        "print('R^2 = ', r2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8baa684",
      "metadata": {
        "id": "b8baa684"
      },
      "source": [
        "### Caso 4. Utiliza K-Fold validación cruzada y la funcion cross validate\n",
        "\n",
        "1. Investiga la función cross_validate de sklearn\n",
        "2. Utilizala para evaluar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a21c7b",
      "metadata": {
        "id": "c5a21c7b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tu código aqui"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f0b1a3",
      "metadata": {
        "id": "12f0b1a3"
      },
      "source": [
        "### Caso 5. Utiliza cross_val_predict function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3799fb",
      "metadata": {
        "id": "3d3799fb"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "diabetes = load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "print(f\"# features: {n_features}\")\n",
        "\n",
        "regr = LinearRegression()\n",
        "y_pred = cross_val_predict(regr, x, y, cv = 5)\n",
        "\n",
        "print(\"y_pred shape: \", y_pred.shape)\n",
        "\n",
        "print('mse = ', mean_squared_error(y, y_pred))\n",
        "print('mae = ', mean_absolute_error(y, y_pred))\n",
        "print('r^2= ', r2_score(y, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32731db8",
      "metadata": {
        "id": "32731db8"
      },
      "source": [
        "# Selección de características utilizando métodos tipo wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0a389b",
      "metadata": {
        "id": "bf0a389b"
      },
      "source": [
        "### Regresion lineal con selección de características sequencial\n",
        "\n",
        "La siguiente celda:\n",
        "\n",
        "1. Selecciona la mitad de las características usando una busqueda sequencial de tipo codiciosa.\n",
        "    - a) Selecciona la mitad de las caracteristicas con SequentialFeatureSelector\n",
        "    - b) Evalúa usando los datos de entrenamiento (no recomendado)\n",
        "    - c) Evalúa utilizando validación cruzada (recomendado).\n",
        "\n",
        "2. Utiliza la selección de características sequencial para decidir el número óptimo de características.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1738485a",
      "metadata": {
        "id": "1738485a"
      },
      "source": [
        "\n",
        "#### Reduce el número de características a la mitad (Modo incorrecto)\n",
        "\n",
        "El problema es que hay data leakage, porque la selección de variables usa todo el dataset antes de evaluar, dando métricas demasiado optimistas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d0ef802",
      "metadata": {
        "id": "5d0ef802"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "print(\"\\n ----- Feature selection using 50% of predictors -----\")\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "fselection = SequentialFeatureSelector(regr, n_features_to_select = 0.5)\n",
        "fselection.fit(x, y)\n",
        "print(\"Selected features: \", fselection.get_feature_names_out())\n",
        "\n",
        "# Fit model using the new dataset and evaluate MSE, MAE and R^2\n",
        "x_transformed = fselection.transform(x)\n",
        "regr.fit(x_transformed, y)\n",
        "print(\"Model coefficients: \", regr.coef_)\n",
        "print(\"Model intercept: \", regr.intercept_)\n",
        "\n",
        "y_pred = regr.predict(x_transformed)\n",
        "print(\"Evaluation using training data (not recommended): \")\n",
        "print('MSE: ', mean_squared_error(y, y_pred))\n",
        "print(\"MAE: \", mean_absolute_error(y, y_pred))\n",
        "print(\"R^2: \", r2_score(y, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79c7b20b",
      "metadata": {
        "id": "79c7b20b"
      },
      "source": [
        "#### Reduce el número de características a la mitad usando validación cruzada (Modo correcto)\n",
        "\n",
        "Este es el modo correcto para evitar fuga de datos. Ya que la selección de características se hace solamente con los datos de entrenamiento. La evaluación se realiza en un conjunto de prueba separado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d175650",
      "metadata": {
        "id": "7d175650"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "mse_cv = []\n",
        "mae_cv = []\n",
        "r2_cv = []\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle = True)\n",
        "\n",
        "for train_index, test_index in kf.split(x):\n",
        "\n",
        "    # Training phase\n",
        "    x_train = x[train_index, :]\n",
        "    y_train = y[train_index]\n",
        "\n",
        "    regr_cv = linear_model.LinearRegression()\n",
        "\n",
        "    fselection_cv = SequentialFeatureSelector(regr_cv, n_features_to_select=0.5)\n",
        "    fselection_cv.fit(x_train, y_train)\n",
        "    x_train = fselection_cv.transform(x_train)\n",
        "\n",
        "    regr_cv.fit(x_train, y_train)\n",
        "\n",
        "    # Test phase\n",
        "    x_test = fselection_cv.transform(x[test_index, :])\n",
        "    y_test = y[test_index]\n",
        "    y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "    mse_i = mean_squared_error(y_test, y_pred)\n",
        "    mse_cv.append(mse_i)\n",
        "\n",
        "    mae_i = mean_absolute_error(y_test, y_pred)\n",
        "    mae_cv.append(mae_i)\n",
        "\n",
        "    r2_i = r2_score(y_test, y_pred)\n",
        "    r2_cv.append(r2_i)\n",
        "\n",
        "print(\"Evaluation using cross-validation (recommended): \")\n",
        "print('MSE:', np.average(mse_cv), '  MAE:', np.average(mae_cv),'  R^2:', np.average(r2_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eb60f82",
      "metadata": {
        "id": "9eb60f82"
      },
      "source": [
        "#### Encuentra el número óptimo de features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f5e0666",
      "metadata": {
        "id": "6f5e0666"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "n_feats = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "mse_nfeat = []\n",
        "mae_nfeat = []\n",
        "r2_nfeat = []\n",
        "for n_feat in n_feats:\n",
        "    print('---- n features =', n_feat)\n",
        "\n",
        "    mse_cv = []\n",
        "    mae_cv = []\n",
        "    r2_cv = []\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle = True)\n",
        "\n",
        "    for train_index, test_index in kf.split(x):\n",
        "\n",
        "        # Training phase\n",
        "        x_train = x[train_index, :]\n",
        "        y_train = y[train_index]\n",
        "\n",
        "        regr_cv = linear_model.LinearRegression()\n",
        "\n",
        "        fselection_cv = SequentialFeatureSelector(regr_cv, n_features_to_select=n_feat)\n",
        "        fselection_cv.fit(x_train, y_train)\n",
        "        x_train = fselection_cv.transform(x_train)\n",
        "\n",
        "        regr_cv.fit(x_train, y_train)\n",
        "\n",
        "        # Test phase\n",
        "        x_test = fselection_cv.transform(x[test_index, :])\n",
        "        y_test = y[test_index]\n",
        "        y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "        mse_i = mean_squared_error(y_test, y_pred)\n",
        "        mse_cv.append(mse_i)\n",
        "\n",
        "        mae_i = mean_absolute_error(y_test, y_pred)\n",
        "        mae_cv.append(mae_i)\n",
        "\n",
        "        r2_i = r2_score(y_test, y_pred)\n",
        "        r2_cv.append(r2_i)\n",
        "\n",
        "\n",
        "    mse = np.average(mse_cv)\n",
        "    mse_nfeat.append(mse)\n",
        "\n",
        "    mae = np.average(mae_cv)\n",
        "    mae_nfeat.append(mae)\n",
        "\n",
        "    r2 = np.average(r2_cv)\n",
        "    r2_nfeat.append(r2)\n",
        "\n",
        "    print('MSE:', mse, '  MAE:', mae,'  R^2:', r2)\n",
        "\n",
        "opt_index = np.argmin(mse_nfeat)\n",
        "opt_features = n_feats[opt_index]\n",
        "print(\"Optimal number of features: \", opt_features)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4), tight_layout=True)\n",
        "\n",
        "axs[0].plot(n_feats, mse_nfeat)\n",
        "axs[0].set_xlabel(\"features\")\n",
        "axs[0].set_ylabel(\"MSE\")\n",
        "\n",
        "axs[1].plot(n_feats, mae_nfeat)\n",
        "axs[1].set_xlabel(\"features\")\n",
        "axs[1].set_ylabel(\"MAE\")\n",
        "\n",
        "axs[2].plot(n_feats, r2_nfeat)\n",
        "axs[2].set_xlabel(\"features\")\n",
        "axs[2].set_ylabel(\"r^2\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Fit model with optimal number of features\n",
        "regr = linear_model.LinearRegression()\n",
        "fselection = SequentialFeatureSelector(regr, n_features_to_select = opt_features)\n",
        "fselection.fit(x, y)\n",
        "\n",
        "print(\"Selected features: \", fselection.get_feature_names_out())\n",
        "\n",
        "x_transformed = fselection.transform(x)\n",
        "regr.fit(x_transformed, y)\n",
        "print(\"Model coefficients: \", regr.coef_)\n",
        "print(\"Model intercept: \", regr.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb4bbe3",
      "metadata": {
        "id": "7bb4bbe3"
      },
      "source": [
        "\n",
        "### Regresion lineal con selección de características regresiva"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2097793d",
      "metadata": {
        "id": "2097793d"
      },
      "source": [
        "1. Empieza con todas las características.\n",
        "2. Entrena un modelo (por ejemplo, LinearRegression, RandomForest, SVC, etc.).\n",
        "3. Calcula la importancia de cada característica (según los coeficientes o atributos del modelo).\n",
        "4. Elimina la(s) característica(s) menos importante(s).\n",
        "5. Repite el proceso recursivamente hasta que queden solo las características deseadas (n_features_to_select).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc5ec80",
      "metadata": {
        "id": "7fc5ec80"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Import Diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "# Evaluate model using cross validation\n",
        "mse_cv = []\n",
        "mae_cv = []\n",
        "r2_cv = []\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle = True)\n",
        "\n",
        "for train_index, test_index in kf.split(x):\n",
        "\n",
        "    # Training phase\n",
        "    x_train = x[train_index, :]\n",
        "    y_train = y[train_index]\n",
        "\n",
        "    regr_cv = linear_model.LinearRegression()\n",
        "\n",
        "    fselection_cv = RFE(regr_cv, n_features_to_select=0.5)\n",
        "    fselection_cv.fit(x_train, y_train)\n",
        "    x_train = fselection_cv.transform(x_train)\n",
        "\n",
        "    regr_cv.fit(x_train, y_train)\n",
        "\n",
        "    # Test phase\n",
        "    x_test = fselection_cv.transform(x[test_index, :])\n",
        "    y_test = y[test_index]\n",
        "    y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "    mse_i = mean_squared_error(y_test, y_pred)\n",
        "    mse_cv.append(mse_i)\n",
        "\n",
        "    mae_i = mean_absolute_error(y_test, y_pred)\n",
        "    mae_cv.append(mae_i)\n",
        "\n",
        "    r2_i = r2_score(y_test, y_pred)\n",
        "    r2_cv.append(r2_i)\n",
        "\n",
        "print('MSE:', np.average(mse_cv), '  MAE:', np.average(mae_cv),'  R^2:', np.average(r2_cv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ad85afd",
      "metadata": {
        "id": "9ad85afd"
      },
      "source": [
        "Encontrar el número óptimo de features usando método recursivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e48da2",
      "metadata": {
        "id": "47e48da2"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import RFE\n",
        "\n",
        "# Import Diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "\n",
        "n_feats = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "mse_nfeat = []\n",
        "mae_nfeat = []\n",
        "r2_nfeat = []\n",
        "for n_feat in n_feats:\n",
        "    print('---- n features =', n_feat)\n",
        "\n",
        "    mse_cv = []\n",
        "    mae_cv = []\n",
        "    r2_cv = []\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle = True)\n",
        "\n",
        "    for train_index, test_index in kf.split(x):\n",
        "\n",
        "        # Training phase\n",
        "        x_train = x[train_index, :]\n",
        "        y_train = y[train_index]\n",
        "\n",
        "        regr_cv = linear_model.LinearRegression()\n",
        "\n",
        "        fselection_cv = RFE(regr_cv, n_features_to_select=n_feat)\n",
        "        fselection_cv.fit(x_train, y_train)\n",
        "        x_train = fselection_cv.transform(x_train)\n",
        "\n",
        "        regr_cv.fit(x_train, y_train)\n",
        "\n",
        "        # Test phase\n",
        "        x_test = fselection_cv.transform(x[test_index, :])\n",
        "        y_test = y[test_index]\n",
        "        y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "        mse_i = mean_squared_error(y_test, y_pred)\n",
        "        mse_cv.append(mse_i)\n",
        "\n",
        "        mae_i = mean_absolute_error(y_test, y_pred)\n",
        "        mae_cv.append(mae_i)\n",
        "\n",
        "        r2_i = r2_score(y_test, y_pred)\n",
        "        r2_cv.append(r2_i)\n",
        "\n",
        "\n",
        "    mse = np.average(mse_cv)\n",
        "    mse_nfeat.append(mse)\n",
        "\n",
        "    mae = np.average(mae_cv)\n",
        "    mae_nfeat.append(mae)\n",
        "\n",
        "    r2 = np.average(r2_cv)\n",
        "    r2_nfeat.append(r2)\n",
        "\n",
        "    print('MSE:', mse, '  MAE:', mae,'  R^2:', r2)\n",
        "\n",
        "opt_index = np.argmin(mse_nfeat)\n",
        "opt_features = n_feats[opt_index]\n",
        "print(\"Optimal number of features: \", opt_features)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4), tight_layout=True)\n",
        "axs[0].plot(n_feats, mse_nfeat)\n",
        "axs[0].set_xlabel(\"features\")\n",
        "axs[0].set_ylabel(\"MSE\")\n",
        "\n",
        "axs[1].plot(n_feats, mae_nfeat)\n",
        "axs[1].set_xlabel(\"features\")\n",
        "axs[1].set_ylabel(\"MAE\")\n",
        "\n",
        "axs[2].plot(n_feats, r2_nfeat)\n",
        "axs[2].set_xlabel(\"features\")\n",
        "axs[2].set_ylabel(\"r^2\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "fselection = RFE(regr, n_features_to_select = opt_features)\n",
        "fselection.fit(x, y)\n",
        "\n",
        "print(\"Selected features: \", fselection.get_feature_names_out())\n",
        "\n",
        "x_transformed = fselection.transform(x)\n",
        "regr.fit(x_transformed, y)\n",
        "print(\"Model coefficients: \", regr.coef_)\n",
        "print(\"Model intercept: \", regr.intercept_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd0d7b7",
      "metadata": {
        "id": "1cd0d7b7"
      },
      "source": [
        "### Regresión lineal con selección de características tipo filter\n",
        "\n",
        "1. Uso de un método filter que utiliza la correlación de pearson para calcular las 5 mejores caracteristicas bajo este criterio.\n",
        "2. Encontrar el número óptimo de características utilizando el método filter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a78d6c4",
      "metadata": {
        "id": "8a78d6c4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SelectKBest, r_regression\n",
        "\n",
        "# Import Diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "x = diabetes.data\n",
        "y = diabetes.target\n",
        "features = diabetes.feature_names\n",
        "n_features = len(features)\n",
        "n_feats = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "mse_nfeat = []\n",
        "mae_nfeat = []\n",
        "r2_nfeat = []\n",
        "for n_feat in n_feats:\n",
        "    print('---- n features =', n_feat)\n",
        "\n",
        "    mse_cv = []\n",
        "    mae_cv = []\n",
        "    r2_cv = []\n",
        "\n",
        "    kf = KFold(n_splits=5, shuffle = True)\n",
        "\n",
        "    for train_index, test_index in kf.split(x):\n",
        "\n",
        "        # Training phase\n",
        "        x_train = x[train_index, :]\n",
        "        y_train = y[train_index]\n",
        "\n",
        "        fselection_cv = SelectKBest(r_regression, k = n_feat)\n",
        "        fselection_cv.fit(x_train, y_train)\n",
        "        x_train = fselection_cv.transform(x_train)\n",
        "\n",
        "        regr_cv = linear_model.LinearRegression()\n",
        "        regr_cv.fit(x_train, y_train)\n",
        "\n",
        "        # Test phase\n",
        "        x_test = fselection_cv.transform(x[test_index, :])\n",
        "        y_test = y[test_index]\n",
        "        y_pred = regr_cv.predict(x_test)\n",
        "\n",
        "        mse_i = mean_squared_error(y_test, y_pred)\n",
        "        mse_cv.append(mse_i)\n",
        "\n",
        "        mae_i = mean_absolute_error(y_test, y_pred)\n",
        "        mae_cv.append(mae_i)\n",
        "\n",
        "        r2_i = r2_score(y_test, y_pred)\n",
        "        r2_cv.append(r2_i)\n",
        "\n",
        "\n",
        "    mse = np.average(mse_cv)\n",
        "    mse_nfeat.append(mse)\n",
        "\n",
        "    mae = np.average(mae_cv)\n",
        "    mae_nfeat.append(mae)\n",
        "\n",
        "    r2 = np.average(r2_cv)\n",
        "    r2_nfeat.append(r2)\n",
        "\n",
        "    print('MSE:', mse, '  MAE:', mae,'  R^2:', r2)\n",
        "\n",
        "opt_index = np.argmin(mse_nfeat)\n",
        "opt_features = n_feats[opt_index]\n",
        "print(\"Optimal number of features: \", opt_features)\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 4), tight_layout=True)\n",
        "axs[0].plot(n_feats, mse_nfeat)\n",
        "axs[0].set_xlabel(\"k\")\n",
        "axs[0].set_ylabel(\"MSE\")\n",
        "\n",
        "axs[1].plot(n_feats, mae_nfeat)\n",
        "axs[1].set_xlabel(\"k\")\n",
        "axs[1].set_ylabel(\"MAE\")\n",
        "\n",
        "axs[2].plot(n_feats, r2_nfeat)\n",
        "axs[2].set_xlabel(\"k\")\n",
        "axs[2].set_ylabel(\"r^2\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Fit model with optimal number of features\n",
        "print(\"\\n ----- Final model with optimal selection of number of features -----\")\n",
        "\n",
        "regr = linear_model.LinearRegression()\n",
        "fselection = SelectKBest(r_regression, k = opt_features)\n",
        "fselection.fit(x, y)\n",
        "\n",
        "print(\"Selected features: \", fselection.get_feature_names_out())\n",
        "\n",
        "x_transformed = fselection.transform(x)\n",
        "regr.fit(x_transformed, y)\n",
        "print(\"Model coefficients: \", regr.coef_)\n",
        "print(\"Model intercept: \", regr.intercept_)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}